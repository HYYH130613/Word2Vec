{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420fe36-094c-4380-9015-ba2798dc7123",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.env (Python 3.12.4)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/Test_Projects/Word2Vec/.env/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from tok import Newname\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "dataset = Newname() \n",
    "tokens = dataset.tokens\n",
    "num_words = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e40ce-a362-41fc-bedb-9812ef2bf4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dim = 10\n",
    "word_vecs = np.concatenate(\n",
    "    (\n",
    "        (np.random.rand(num_words, vector_dim) - 0.5) / vector_dim,\n",
    "        np.zeros(\n",
    "            (num_words, vector_dim)\n",
    "        ),  # for simplicity's sake, we will have a separate set of vectors for each context word as well as for each center word\n",
    "    ),\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "word_vecs.shape  # 2*num_words (one for the context vector and another for the center vector) x vector_dim\n",
    "# initially random vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c162a-6630-435e-9592-7c1ce99702f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_wrapper(model, w2i, word_vecs, dataset, block_size, loss_and_grad):\n",
    "    batch_size = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(word_vecs.shape)\n",
    "    N = word_vecs.shape[0]\n",
    "    center_word_vecs = word_vecs[: int(N / 2), :]\n",
    "    outside_word_vecs = word_vecs[int(N / 2) :, :]\n",
    "    for i in range(batch_size):\n",
    "        block_size1 = random.randint(1, block_size)\n",
    "        center_word, context = dataset.get_random_context(block_size1)\n",
    "\n",
    "        c, grad_in, grad_out = model(\n",
    "            center_word,\n",
    "            block_size1,\n",
    "            context,\n",
    "            w2i,\n",
    "            center_word_vecs,\n",
    "            outside_word_vecs,\n",
    "            dataset,\n",
    "            loss_and_grad,\n",
    "        )\n",
    "        loss += c / batch_size\n",
    "        grad[: int(N / 2), :] += grad_in / batch_size\n",
    "        grad[int(N / 2) :, :] += grad_out / batch_size\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd90cc3-8cac-42cd-a0c2-274f7eb76d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_wrapper(model, w2i, word_vecs, dataset, block_size, loss_and_grad):\n",
    "    batch_size = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(word_vecs.shape)\n",
    "    N = word_vecs.shape[0]\n",
    "    center_word_vecs = word_vecs[: int(N / 2), :]\n",
    "    outside_word_vecs = word_vecs[int(N / 2) :, :]\n",
    "    for i in range(batch_size):\n",
    "        block_size1 = random.randint(1, block_size)\n",
    "        center_word, context = dataset.get_random_context(block_size1)\n",
    "\n",
    "        c, grad_in, grad_out = model(\n",
    "            center_word,\n",
    "            block_size1,\n",
    "            context,\n",
    "            w2i,\n",
    "            center_word_vecs,\n",
    "            outside_word_vecs,\n",
    "            dataset,\n",
    "            loss_and_grad,\n",
    "        )\n",
    "        loss += c / batch_size\n",
    "        grad[: int(N / 2), :] += grad_in / batch_size\n",
    "        grad[int(N / 2) :, :] += grad_out / batch_size\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0794595-7db6-4992-bb21-cf78d4f5d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3d4d3-df81-45ad-a2d4-bf6f8a05d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax function for each row of the input x.\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        tmp = np.max(x, axis=1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis=1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))\n",
    "    else:\n",
    "        # Vector\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9847d-910b-4a9d-82b4-5acaceb7a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal softmax\n",
    "def softmaxloss_gradient(center_word_vec, outside_word_idx, outside_word_vecs, dataset):\n",
    "    dot_products = np.dot(outside_word_vecs, center_word_vec)\n",
    "    softmax_probs = softmax(dot_products)\n",
    "    loss = -np.log(softmax_probs[outside_word_idx])\n",
    "\n",
    "    grad_center_vec = -outside_word_vecs[outside_word_idx] + np.dot(\n",
    "        softmax_probs, outside_word_vecs\n",
    "    )\n",
    "    grad_outside_vecs = np.outer(softmax_probs, center_word_vec)\n",
    "    grad_outside_vecs[outside_word_idx] -= center_word_vec\n",
    "\n",
    "    return loss, grad_center_vec, grad_outside_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216ded1-bfb3-47f9-8614-6577e36a0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(outsideWordIdx, dataset, K):\n",
    "    \"\"\"Samples K indexes which are not the outsideWordIdx\"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return [int(n) for n in negSampleWordIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732afa5-8cfb-4593-ba40-3254edee0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative sampling\n",
    "def negative_samplingloss_gradient(\n",
    "    center_word_vec, outside_word_idx, outside_word_vecs, dataset, K=10\n",
    "):\n",
    "    neg_samples = get_negative_samples(outside_word_idx, dataset, K)\n",
    "\n",
    "    grad_center_vec = np.zeros(center_word_vec.shape)\n",
    "    grad_outside_vecs = np.zeros(outside_word_vecs.shape)\n",
    "\n",
    "    u_0 = outside_word_vecs[outside_word_idx]\n",
    "    z_0 = np.dot(u_0, center_word_vec)\n",
    "    p_0 = sigmoid(z_0)\n",
    "    loss = -np.log(p_0)\n",
    "\n",
    "    grad_center_vec += (p_0 - 1) * u_0\n",
    "    grad_outside_vecs[outside_word_idx] += (p_0 - 1) * center_word_vec\n",
    "\n",
    "    for k in neg_samples:\n",
    "        u_k = outside_word_vecs[k]\n",
    "        z_k = np.dot(u_k, center_word_vec)\n",
    "        p_k = sigmoid(-z_k)\n",
    "        loss -= np.log(p_k)\n",
    "\n",
    "        grad_center_vec -= (p_k - 1) * u_k\n",
    "        grad_outside_vecs[k] -= (p_k - 1) * center_word_vec\n",
    "\n",
    "    return loss, grad_center_vec, grad_outside_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad90bac-6279-4028-b838-3c240db982d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(\n",
    "    current_center_word,\n",
    "    block_size1,\n",
    "    outside_words,\n",
    "    w2i,\n",
    "    center_word_vecs,\n",
    "    outside_word_vecs,\n",
    "    dataset,\n",
    "    loss_and_grad,\n",
    "):\n",
    "    loss = 0.0\n",
    "    grad_center_vecs = np.zeros(center_word_vecs.shape)\n",
    "    grad_outside_vecs = np.zeros(outside_word_vecs.shape)\n",
    "\n",
    "    center_word_idx = w2i[current_center_word]\n",
    "    center_word_vec = center_word_vecs[center_word_idx]\n",
    "\n",
    "    for outside_word in outside_words:\n",
    "        outside_word_idx = w2i[outside_word]\n",
    "        current_loss, current_grad_center_vec, current_grad_outside_vecs = (\n",
    "            loss_and_grad(center_word_vec, outside_word_idx, outside_word_vecs, dataset)\n",
    "        )\n",
    "        loss += current_loss\n",
    "        grad_center_vecs[center_word_idx] += current_grad_center_vec\n",
    "        grad_outside_vecs += current_grad_outside_vecs\n",
    "\n",
    "    return loss, grad_center_vecs, grad_outside_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8664fe-6012-4ee0-a133-685a132aa354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "\n",
    "SAVE_PARAMS_EVERY = 2000\n",
    "\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\"\n",
    "    A helper function that loads previously saved parameters and resets\n",
    "    iteration start.\n",
    "    \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if iter > st:\n",
    "            st = iter\n",
    "\n",
    "    if st > 0:\n",
    "        params_file = \"saved_params_%d.npy\" % st\n",
    "        state_file = \"saved_state_%d.pickle\" % st\n",
    "        params = np.load(params_file)\n",
    "        with open(state_file, \"rb\") as f:\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "\n",
    "\n",
    "def save_params(iter, params):\n",
    "    params_file = \"saved_params_%d.npy\" % iter\n",
    "    np.save(params_file, params)\n",
    "    with open(\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "def sgd(f, x0, step, iterations, use_saved=False, PRINT_EVERY=10):\n",
    "    ANNEAL_EVERY = 5000\n",
    "    if use_saved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step = 0.0\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "    exploss = None\n",
    "\n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "        loss = None\n",
    "        loss, gradient = f(x)\n",
    "        x -= step * gradient\n",
    "\n",
    "        if exploss is None:\n",
    "            exploss = loss\n",
    "        else:\n",
    "            exploss = 0.95 * exploss + 0.05 * loss\n",
    "\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not exploss:\n",
    "                exploss = loss\n",
    "            else:\n",
    "                exploss = 0.95 * exploss + 0.05 * loss\n",
    "            print(\"iter %d: %f\" % (iter, exploss))\n",
    "            losses.append(exploss)\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and use_saved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e2f9d-80e5-432b-b1de-6421e8855c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vector_dim = 10\n",
    "C = 5\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "start_time = time.time()\n",
    "word_vecs = np.concatenate(\n",
    "    (\n",
    "        (np.random.rand(num_words, vector_dim) - 0.5) / vector_dim,\n",
    "        np.zeros((num_words, vector_dim)),\n",
    "    ),\n",
    "    axis=0,\n",
    ")\n",
    "word_vecs = sgd(\n",
    "    lambda vec: w2v_wrapper(\n",
    "        skipgram, tokens, vec, dataset, C, negative_samplingloss_gradient\n",
    "    ),\n",
    "    word_vecs,\n",
    "    step=1,  # was .01\n",
    "    iterations=10000,\n",
    "    use_saved=True,\n",
    "    PRINT_EVERY=10,\n",
    ")\n",
    "\n",
    "print(\"training took %d seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995298ad-06ff-4385-bc27-a8c89011312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import makeLogRecord\n",
    "\n",
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs. Iterations\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "makeLogRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d999c6f-015e-4e65-9779-d6583b93f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_word_vectors = np.concatenate(\n",
    "    (word_vecs[:num_words, :], word_vecs[num_words:, :]), axis=0\n",
    ")  # put all of center word vecs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75068e5-2697-4a14-970a-f12fc3c67318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_words = [\n",
    "#     \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "#     \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n",
    "#     \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n",
    "#     \"hail\", \"coffee\", \"tea\"]\n",
    "\n",
    "visualize_words = [\"Valkyria\", \"characters\", \"unvoiced\", \"heroines\"]  # analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ba591-f18f-4e47-9866-28eac555a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_idx = [tokens[word.lower()] for word in visualize_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf130c-94ef-434a-80e6-b20b4144f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction for visualization\n",
    "visualize_vecs = trained_word_vectors[visualize_idx, :]\n",
    "temp = visualize_vecs - np.mean(visualize_vecs, axis=0)\n",
    "covariance = 1.0 / len(visualize_idx) * temp.T.dot(temp)\n",
    "U, S, V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:, 0:2])\n",
    "full_piece = temp.dot(U[:, 0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb9ee85-2d14-4035-b887-d44b3ad33022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(visualize_words)):\n",
    "    plt.text(\n",
    "        coord[i, 0],\n",
    "        coord[i, 1],\n",
    "        visualize_words[i],\n",
    "        bbox=dict(facecolor=\"green\", alpha=0.1),\n",
    "    )\n",
    "\n",
    "plt.xlim((np.min(coord[:, 0]), np.max(coord[:, 0])))\n",
    "plt.ylim((np.min(coord[:, 1]), np.max(coord[:, 1])))\n",
    "plt.show()\n",
    "makeLogRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188f16d-fdca-469f-85db-ee5f8905abca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
